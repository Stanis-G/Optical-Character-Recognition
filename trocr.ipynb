{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "from utils import inference, save_model_and_history, evaluate_model, cer_score, plot_history\n",
    "# from OCR_VQA.data_preparation import VQAProcessor\n",
    "from custom_dataset.data_preparation import CustomDataProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrOCRProcessor class wraps image processor class and tokenizer class\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-printed\")\n",
    "\n",
    "data_processor = CustomDataProcessor(processor)\n",
    "train_dataset, val_dataset, test_dataset, train_size = data_processor(dataset_batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing functions\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    output_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return output_ids, labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    output_ids, labels_ids = eval_pred\n",
    "    words_predicted = processor.tokenizer.batch_decode(output_ids[0], skip_special_tokens=False)\n",
    "    words_labels = processor.tokenizer.batch_decode(labels_ids, skip_special_tokens=False)\n",
    "    return {'cer': cer_score.compute(predictions=words_predicted, references=words_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-printed\")\n",
    "\n",
    "model.config.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "\n",
    "model_name = '04.02.25_12787_v1'\n",
    "num_epochs = 30\n",
    "batch_size = 8\n",
    "max_steps = int((train_size / batch_size) * num_epochs)\n",
    "eval_steps = logging_steps = 1000\n",
    "\n",
    "training_hyperparams = TrainingArguments(\n",
    "    output_dir=f'trocr_checkpoints/{model_name}',\n",
    "    eval_strategy='steps', # evaluate on eval_dataset every eval_steps\n",
    "    eval_steps=eval_steps,\n",
    "    eval_accumulation_steps=logging_steps,\n",
    "    logging_steps=logging_steps, # update steps to perform before output logs\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    # num_train_epochs=num_epochs,\n",
    "    save_total_limit=1, # Save only last checkpoint\n",
    "    load_best_model_at_end=True, # Save best model\n",
    "    # metric_for_best_model=cer_score, # Metric to evaluate checnkpoints\n",
    "    # greater_is_better=True,\n",
    "    save_steps=10000,\n",
    "    # logging_dir='trocr_checkpoints/logs',\n",
    "    max_steps=max_steps, # Overrides num_train_epochs\n",
    "    # fp16=True,\n",
    "    fp16_full_eval=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "if not os.path.exists(f'trocr_checkpoints/{model_name}'):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_hyperparams,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "        compute_metrics=compute_metrics,\n",
    "        # processing_class=processor,\n",
    "        # data_collator=...\n",
    "    )\n",
    "    trainer.train()\n",
    "else:\n",
    "    raise ValueError(f\"Model '{model_name}' exists, specify anothe name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save model and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_history(model_name, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Plot training history__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i['epoch'] for i in trainer.state.log_history if 'eval_cer' in i]\n",
    "train_loss = [i['loss'] for i in trainer.state.log_history if 'loss' in i]\n",
    "val_loss = [i['eval_loss'] for i in trainer.state.log_history if 'eval_loss' in i]\n",
    "val_cer = [10 * i['eval_cer'] for i in trainer.state.log_history if 'eval_cer' in i]\n",
    "\n",
    "hist = {'train_loss': train_loss, 'val_loss': val_loss, 'val_cer * 10': val_cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(\n",
    "    epochs,\n",
    "    hist,\n",
    "    run_name=model_name,\n",
    "    figsize=(15, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Load model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '04.02.25_12787_v1'\n",
    "\n",
    "model_path = f'models/{model_name}/model'\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Evaluate model on datasets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, train_cer_value = evaluate_model(model, processor, train_dataset.dataset.indeces, cer_score)\n",
    "train_cer_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, val_cer_value = evaluate_model(model, processor, val_dataset.dataset.indeces, cer_score)\n",
    "val_cer_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_cer_value = evaluate_model(model, processor, test_dataset.dataset.indeces, cer_score)\n",
    "test_cer_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Inference on new images__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_fold = 'test_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, text = inference(f'{image_fold}/test_screen.png', model, processor)\n",
    "print(text)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, text = inference(f'{image_fold}/one_channel_image.jpg', model, processor)\n",
    "print(text)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, text = inference(f'{image_fold}/test_screen_2.png', model, processor)\n",
    "print(text)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Inference on images from dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_fold = 'custom_dataset/data/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "\n",
    "img, text = inference(f'{image_fold}/image_{idx}.png', model, processor)\n",
    "print(text)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 35\n",
    "\n",
    "img, text = inference(f'{image_fold}/image_{idx}.png', model, processor)\n",
    "print(text)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "\n",
    "img, text = inference(f'{image_fold}/image_{idx}.png', model, processor)\n",
    "print(text)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
